{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent GNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michaels/anaconda3/envs/icgnn/lib/python3.11/site-packages/torch_geometric/typing.py:68: UserWarning: An issue occurred while importing 'pyg-lib'. Disabling its usage. Stacktrace: dlopen(/Users/michaels/anaconda3/envs/icgnn/lib/python3.11/site-packages/libpyg.so, 0x0006): Library not loaded: /Library/Frameworks/Python.framework/Versions/3.11/Python\n",
      "  Referenced from: <CA14ED34-FA3D-31FE-B4AD-2B2A8446B324> /Users/michaels/anaconda3/envs/icgnn/lib/python3.11/site-packages/libpyg.so\n",
      "  Reason: tried: '/Library/Frameworks/Python.framework/Versions/3.11/Python' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/Library/Frameworks/Python.framework/Versions/3.11/Python' (no such file), '/Library/Frameworks/Python.framework/Versions/3.11/Python' (no such file)\n",
      "  warnings.warn(f\"An issue occurred while importing 'pyg-lib'. \"\n",
      "/Users/michaels/anaconda3/envs/icgnn/lib/python3.11/site-packages/torch_geometric/typing.py:124: UserWarning: An issue occurred while importing 'torch-sparse'. Disabling its usage. Stacktrace: dlopen(/Users/michaels/anaconda3/envs/icgnn/lib/python3.11/site-packages/libpyg.so, 0x0006): Library not loaded: /Library/Frameworks/Python.framework/Versions/3.11/Python\n",
      "  Referenced from: <CA14ED34-FA3D-31FE-B4AD-2B2A8446B324> /Users/michaels/anaconda3/envs/icgnn/lib/python3.11/site-packages/libpyg.so\n",
      "  Reason: tried: '/Library/Frameworks/Python.framework/Versions/3.11/Python' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/Library/Frameworks/Python.framework/Versions/3.11/Python' (no such file), '/Library/Frameworks/Python.framework/Versions/3.11/Python' (no such file)\n",
      "  warnings.warn(f\"An issue occurred while importing 'torch-sparse'. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MPS device: mps\n"
     ]
    }
   ],
   "source": [
    "# Importing necessary modules and libraries\n",
    "import os.path as osp  # For handling file and directory paths\n",
    "import torch  # PyTorch library for tensor computations and deep learning\n",
    "import torch.nn as nn  # Neural network module in PyTorch\n",
    "import torch.nn.functional as F  # Functional interface for PyTorch (e.g., activation functions)\n",
    "import torch_geometric.transforms as T  # Transformations for graph data in PyTorch Geometric\n",
    "import torch_geometric  # PyTorch Geometric library for graph-based deep learning\n",
    "from torch_geometric.datasets import Planetoid, TUDataset  # Datasets for graph learning tasks\n",
    "from torch_geometric.loader import DataLoader  # DataLoader for batching graph data\n",
    "from torch_geometric.nn.inits import uniform  # Initialization utility for PyTorch Geometric\n",
    "from torch.nn import Parameter as Param  # Parameter class for defining learnable parameters\n",
    "from torch import Tensor  # Tensor class for type hinting\n",
    "\n",
    "# Set the random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Check if the Metal Performance Shaders (MPS) backend is available for GPU acceleration on macOS\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")  # Use MPS device if available\n",
    "    print(\"Using MPS device:\", device)\n",
    "else:\n",
    "    device = torch.device(\"cpu\")  # Fallback to CPU if MPS is not available\n",
    "    print(\"MPS not available, using CPU\")\n",
    "\n",
    "# Importing the MessagePassing class from PyTorch Geometric\n",
    "# This class is used to define custom message-passing layers for graph neural networks\n",
    "from torch_geometric.nn.conv import MessagePassing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dataset name as 'Cora', which is a commonly used citation network dataset\n",
    "dataset = 'Cora'\n",
    "\n",
    "# Define a series of transformations to be applied to the dataset\n",
    "# T.RandomNodeSplit: Splits the nodes into training, validation, and test sets.\n",
    "#   'train_rest': Specifies that the remaining nodes after validation and test splits will be used for training.\n",
    "#   num_val=500: Specifies that 500 nodes will be used for validation.\n",
    "#   num_test=500: Specifies that 500 nodes will be used for testing.\n",
    "# T.TargetIndegree: Adds the in-degree of each node as a feature to the dataset.\n",
    "transform = T.Compose([\n",
    "    T.RandomNodeSplit('train_rest', num_val=500, num_test=500),\n",
    "    T.TargetIndegree(),\n",
    "])\n",
    "\n",
    "# Define the path where the dataset will be stored or loaded from\n",
    "# osp.join: Joins the directory name ('data') and the dataset name ('Cora') into a single path\n",
    "path = osp.join('data', dataset)\n",
    "\n",
    "# Load the Planetoid dataset with the specified path and transformations\n",
    "# Planetoid: A dataset class in PyTorch Geometric for citation network datasets like Cora, Citeseer, and PubMed\n",
    "# transform=transform: Applies the defined transformations to the dataset\n",
    "dataset = Planetoid(path, dataset, transform=transform)\n",
    "\n",
    "# Access the first graph in the dataset (Cora contains only one graph)\n",
    "data = dataset[0]\n",
    "data = data.to(device)  # Move the graph data to the specified device (CPU or MPS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph Neural Network Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MLP class is used to instantiate the transition and output functions as simple feed forard networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \"\"\"\n",
    "    A Multi-Layer Perceptron (MLP) class that defines a simple feed-forward neural network.\n",
    "    This class allows for the creation of an MLP with customizable input, hidden, and output dimensions.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hid_dims, out_dim):\n",
    "        \"\"\"\n",
    "        Initializes the MLP class.\n",
    "\n",
    "        Args:\n",
    "            input_dim (int): The number of input features.\n",
    "            hid_dims (list of int): A list containing the number of units in each hidden layer.\n",
    "            out_dim (int): The number of output features.\n",
    "        \"\"\"\n",
    "        super(MLP, self).__init__()  # Call the parent class (nn.Module) initializer.\n",
    "\n",
    "        # Define a sequential container to hold the layers of the MLP.\n",
    "        self.mlp = nn.Sequential()\n",
    "\n",
    "        # Combine input, hidden, and output dimensions into a single list for layer construction.\n",
    "        dims = [input_dim] + hid_dims + [out_dim]\n",
    "\n",
    "        # Loop through the dimensions to create layers.\n",
    "        for i in range(len(dims) - 1):\n",
    "            # Add a linear layer (fully connected layer) to the MLP.\n",
    "            self.mlp.add_module('lay_{}'.format(i), nn.Linear(in_features=dims[i], out_features=dims[i + 1]))\n",
    "\n",
    "            # Add an activation function (Tanh) after each linear layer, except the last one.\n",
    "            if i + 2 < len(dims):  # Skip adding activation for the last layer.\n",
    "                self.mlp.add_module('act_{}'.format(i), nn.Tanh())\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        \"\"\"\n",
    "        Resets the parameters of the MLP layers.\n",
    "        Specifically, it initializes the weights of all linear layers using Xavier normal initialization.\n",
    "        \"\"\"\n",
    "        for i, l in enumerate(self.mlp):  # Iterate through all layers in the sequential container.\n",
    "            if type(l) == nn.Linear:  # Check if the layer is a linear layer.\n",
    "                nn.init.xavier_normal_(l.weight)  # Apply Xavier normal initialization to the weights.\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Defines the forward pass of the MLP.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): The input tensor to the MLP.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: The output tensor after passing through the MLP.\n",
    "        \"\"\"\n",
    "        return self.mlp(x)  # Pass the input through the sequential container (MLP).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The GNNM calss puts together the state propagations and the readout of the nodes' states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNNM(MessagePassing):\n",
    "    \"\"\"\n",
    "    A Graph Neural Network Model (GNNM) class that extends the MessagePassing class from PyTorch Geometric.\n",
    "    This class implements a recurrent graph neural network with state propagation and readout mechanisms.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_nodes, out_channels, features_dim, hid_dims, num_layers=50, eps=1e-3, aggr='add',\n",
    "                 bias=True, **kwargs):\n",
    "        \"\"\"\n",
    "        Initializes the GNNM class.\n",
    "\n",
    "        Args:\n",
    "            n_nodes (int): The number of nodes in the graph.\n",
    "            out_channels (int): The number of output channels (e.g., number of classes for classification).\n",
    "            features_dim (int): The dimensionality of the node features.\n",
    "            hid_dims (list of int): A list of hidden layer dimensions for the transition and readout MLPs.\n",
    "            num_layers (int, optional): The maximum number of propagation layers. Default is 50.\n",
    "            eps (float, optional): Convergence threshold for node state updates. Default is 1e-3.\n",
    "            aggr (str, optional): Aggregation method for message passing ('add', 'mean', 'max'). Default is 'add'.\n",
    "            bias (bool, optional): Whether to include bias in the MLP layers. Default is True.\n",
    "            **kwargs: Additional arguments for the MessagePassing class.\n",
    "        \"\"\"\n",
    "        super(GNNM, self).__init__(aggr=aggr, **kwargs)  # Initialize the parent MessagePassing class.\n",
    "\n",
    "        # Initialize node states as a learnable parameter (not updated during backpropagation).\n",
    "        self.node_states = Param(torch.zeros((n_nodes, features_dim), device=device), requires_grad=False)\n",
    "\n",
    "        # Number of output channels (e.g., number of classes for classification).\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "        # Convergence threshold for node state updates.\n",
    "        self.eps = eps\n",
    "\n",
    "        # Maximum number of propagation layers.\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # Transition function: A Multi-Layer Perceptron (MLP) for updating node states.\n",
    "        self.transition = MLP(features_dim, hid_dims, features_dim)\n",
    "\n",
    "        # Readout function: A Multi-Layer Perceptron (MLP) for producing the final output.\n",
    "        self.readout = MLP(features_dim, hid_dims, out_channels)\n",
    "\n",
    "        # Initialize the parameters of the transition and readout MLPs.\n",
    "        self.reset_parameters()\n",
    "\n",
    "        # Print the architectures of the transition and readout MLPs for debugging purposes.\n",
    "        print(self.transition)\n",
    "        print(self.readout)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        \"\"\"\n",
    "        Resets the parameters of the transition and readout MLPs.\n",
    "        This ensures that the weights are initialized properly before training.\n",
    "        \"\"\"\n",
    "        self.transition.reset_parameters()\n",
    "        self.readout.reset_parameters()\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Defines the forward pass of the GNNM.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: The log-softmax output of the model for each node.\n",
    "        \"\"\"\n",
    "        # Access the edge index (connectivity information) and edge attributes from the dataset.\n",
    "        edge_index = data.edge_index\n",
    "        edge_weight = data.edge_attr\n",
    "\n",
    "        # Initialize the node states.\n",
    "        node_states = self.node_states\n",
    "\n",
    "        # Perform message passing and state updates for a maximum of `num_layers` iterations.\n",
    "        for i in range(self.num_layers):\n",
    "            # Propagate messages across the graph.\n",
    "            # `propagate` is a method from the MessagePassing class that handles message passing.\n",
    "            m = self.propagate(edge_index, x=node_states, edge_weight=edge_weight, size=None)\n",
    "\n",
    "            # Update the node states using the transition MLP.\n",
    "            new_states = self.transition(m)\n",
    "\n",
    "            # Compute the distance between the new and old node states to check for convergence.\n",
    "            with torch.no_grad():  # Disable gradient computation for this block.\n",
    "                distance = torch.norm(new_states - node_states, dim=1)  # Compute the L2 norm for each node.\n",
    "                convergence = distance < self.eps  # Check if the distance is below the convergence threshold.\n",
    "\n",
    "            # Update the node states.\n",
    "            node_states = new_states\n",
    "\n",
    "            # If all nodes have converged, stop the propagation early.\n",
    "            if convergence.all():\n",
    "                break\n",
    "\n",
    "        # Apply the readout MLP to the final node states to produce the output.\n",
    "        out = self.readout(node_states)\n",
    "\n",
    "        # Apply log-softmax to the output for classification tasks.\n",
    "        return F.log_softmax(out, dim=-1)\n",
    "\n",
    "    def message(self, x_j, edge_weight):\n",
    "        \"\"\"\n",
    "        Defines the message computation for each edge.\n",
    "\n",
    "        Args:\n",
    "            x_j (Tensor): The features of the neighboring nodes.\n",
    "            edge_weight (Tensor, optional): The weights of the edges.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: The computed messages for each edge.\n",
    "        \"\"\"\n",
    "        # If edge weights are provided, scale the neighboring node features by the edge weights.\n",
    "        # Otherwise, return the neighboring node features as-is.\n",
    "        return x_j if edge_weight is None else edge_weight.view(-1, 1) * x_j\n",
    "\n",
    "    def message_and_aggregate(self, adj_t, x):\n",
    "        \"\"\"\n",
    "        Defines the combined message and aggregation step for sparse adjacency matrices.\n",
    "\n",
    "        Args:\n",
    "            adj_t (SparseTensor): The transposed adjacency matrix.\n",
    "            x (Tensor): The node features.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: The aggregated messages for each node.\n",
    "        \"\"\"\n",
    "        # Perform sparse matrix multiplication to aggregate messages.\n",
    "        return matmul(adj_t, x, reduce=self.aggr)\n",
    "\n",
    "    def __repr__(self):\n",
    "        \"\"\"\n",
    "        Returns a string representation of the GNNM class.\n",
    "\n",
    "        Returns:\n",
    "            str: A string describing the class, output channels, and number of layers.\n",
    "        \"\"\"\n",
    "        return '{}({}, num_layers={})'.format(self.__class__.__name__,\n",
    "                                              self.out_channels,\n",
    "                                              self.num_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (mlp): Sequential(\n",
      "    (lay_0): Linear(in_features=32, out_features=64, bias=True)\n",
      "    (act_0): Tanh()\n",
      "    (lay_1): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (act_1): Tanh()\n",
      "    (lay_2): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (act_2): Tanh()\n",
      "    (lay_3): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (act_3): Tanh()\n",
      "    (lay_4): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (act_4): Tanh()\n",
      "    (lay_5): Linear(in_features=64, out_features=32, bias=True)\n",
      "  )\n",
      ")\n",
      "MLP(\n",
      "  (mlp): Sequential(\n",
      "    (lay_0): Linear(in_features=32, out_features=64, bias=True)\n",
      "    (act_0): Tanh()\n",
      "    (lay_1): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (act_1): Tanh()\n",
      "    (lay_2): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (act_2): Tanh()\n",
      "    (lay_3): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (act_3): Tanh()\n",
      "    (lay_4): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (act_4): Tanh()\n",
      "    (lay_5): Linear(in_features=64, out_features=7, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michaels/anaconda3/envs/icgnn/lib/python3.11/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Train Acc: 0.16393, Val Acc: 0.15400, Test Acc: 0.13800\n",
      "Epoch: 002, Train Acc: 0.29859, Val Acc: 0.28800, Test Acc: 0.32400\n",
      "Epoch: 003, Train Acc: 0.29859, Val Acc: 0.29600, Test Acc: 0.32000\n",
      "Epoch: 004, Train Acc: 0.29801, Val Acc: 0.29600, Test Acc: 0.31800\n",
      "Epoch: 005, Train Acc: 0.29801, Val Acc: 0.29800, Test Acc: 0.31800\n",
      "Epoch: 006, Train Acc: 0.30094, Val Acc: 0.29600, Test Acc: 0.32000\n",
      "Epoch: 007, Train Acc: 0.30269, Val Acc: 0.29800, Test Acc: 0.32200\n",
      "Epoch: 008, Train Acc: 0.30386, Val Acc: 0.29600, Test Acc: 0.32200\n",
      "Epoch: 009, Train Acc: 0.30445, Val Acc: 0.29200, Test Acc: 0.32000\n",
      "Epoch: 010, Train Acc: 0.30152, Val Acc: 0.28800, Test Acc: 0.32400\n",
      "Epoch: 011, Train Acc: 0.30152, Val Acc: 0.29000, Test Acc: 0.31600\n",
      "Epoch: 012, Train Acc: 0.30269, Val Acc: 0.28800, Test Acc: 0.31400\n",
      "Epoch: 013, Train Acc: 0.30386, Val Acc: 0.29200, Test Acc: 0.31800\n",
      "Epoch: 014, Train Acc: 0.30621, Val Acc: 0.28800, Test Acc: 0.32200\n",
      "Epoch: 015, Train Acc: 0.30386, Val Acc: 0.29000, Test Acc: 0.32200\n",
      "Epoch: 016, Train Acc: 0.30386, Val Acc: 0.29400, Test Acc: 0.32400\n",
      "Epoch: 017, Train Acc: 0.30094, Val Acc: 0.29600, Test Acc: 0.31600\n",
      "Epoch: 018, Train Acc: 0.30328, Val Acc: 0.29400, Test Acc: 0.32000\n",
      "Epoch: 019, Train Acc: 0.31206, Val Acc: 0.32000, Test Acc: 0.34000\n",
      "Epoch: 020, Train Acc: 0.30035, Val Acc: 0.29400, Test Acc: 0.32200\n",
      "Epoch: 021, Train Acc: 0.29859, Val Acc: 0.28600, Test Acc: 0.32600\n",
      "Epoch: 022, Train Acc: 0.29918, Val Acc: 0.28600, Test Acc: 0.32600\n",
      "Epoch: 023, Train Acc: 0.29801, Val Acc: 0.29000, Test Acc: 0.32800\n",
      "Epoch: 024, Train Acc: 0.29859, Val Acc: 0.29600, Test Acc: 0.32000\n",
      "Epoch: 025, Train Acc: 0.29859, Val Acc: 0.29600, Test Acc: 0.32000\n",
      "Epoch: 026, Train Acc: 0.29859, Val Acc: 0.29600, Test Acc: 0.32000\n",
      "Epoch: 027, Train Acc: 0.29859, Val Acc: 0.29600, Test Acc: 0.32000\n",
      "Epoch: 028, Train Acc: 0.29859, Val Acc: 0.29600, Test Acc: 0.31800\n",
      "Epoch: 029, Train Acc: 0.29859, Val Acc: 0.29600, Test Acc: 0.31800\n",
      "Epoch: 030, Train Acc: 0.29742, Val Acc: 0.29600, Test Acc: 0.31800\n",
      "Epoch: 031, Train Acc: 0.29859, Val Acc: 0.29200, Test Acc: 0.32000\n",
      "Epoch: 032, Train Acc: 0.30269, Val Acc: 0.28600, Test Acc: 0.32200\n",
      "Epoch: 033, Train Acc: 0.30386, Val Acc: 0.28600, Test Acc: 0.31800\n",
      "Epoch: 034, Train Acc: 0.29977, Val Acc: 0.28400, Test Acc: 0.31800\n",
      "Epoch: 035, Train Acc: 0.29391, Val Acc: 0.29200, Test Acc: 0.31600\n",
      "Epoch: 036, Train Acc: 0.29157, Val Acc: 0.29400, Test Acc: 0.31600\n",
      "Epoch: 037, Train Acc: 0.29215, Val Acc: 0.29200, Test Acc: 0.31800\n",
      "Epoch: 038, Train Acc: 0.29567, Val Acc: 0.29400, Test Acc: 0.32000\n",
      "Epoch: 039, Train Acc: 0.29684, Val Acc: 0.29600, Test Acc: 0.32000\n",
      "Epoch: 040, Train Acc: 0.30035, Val Acc: 0.29600, Test Acc: 0.31800\n",
      "Epoch: 041, Train Acc: 0.29977, Val Acc: 0.29400, Test Acc: 0.32000\n",
      "Epoch: 042, Train Acc: 0.30152, Val Acc: 0.29200, Test Acc: 0.32000\n",
      "Epoch: 043, Train Acc: 0.29859, Val Acc: 0.28600, Test Acc: 0.31600\n",
      "Epoch: 044, Train Acc: 0.29977, Val Acc: 0.28600, Test Acc: 0.31600\n",
      "Epoch: 045, Train Acc: 0.30094, Val Acc: 0.28800, Test Acc: 0.31800\n",
      "Epoch: 046, Train Acc: 0.30269, Val Acc: 0.29200, Test Acc: 0.32000\n",
      "Epoch: 047, Train Acc: 0.30211, Val Acc: 0.29000, Test Acc: 0.32200\n",
      "Epoch: 048, Train Acc: 0.30035, Val Acc: 0.28800, Test Acc: 0.32000\n",
      "Epoch: 049, Train Acc: 0.30152, Val Acc: 0.28800, Test Acc: 0.32000\n",
      "Epoch: 050, Train Acc: 0.30269, Val Acc: 0.28600, Test Acc: 0.31800\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the GNNM model with the specified parameters:\n",
    "# - data.num_nodes: Number of nodes in the graph.\n",
    "# - dataset.num_classes: Number of output classes for classification.\n",
    "# - 32: Dimensionality of the node features.\n",
    "# - [64, 64, 64, 64, 64]: Hidden layer dimensions for the transition and readout MLPs.\n",
    "# - eps=0.01: Convergence threshold for node state updates.\n",
    "# Move the model to the specified device (CPU or MPS).\n",
    "model = GNNM(data.num_nodes, dataset.num_classes, 32, [64, 64, 64, 64, 64], eps=0.01).to(device)\n",
    "\n",
    "# Define the optimizer for training the model.\n",
    "# - torch.optim.Adam: Adam optimizer, which is a popular optimization algorithm for deep learning.\n",
    "# - model.parameters(): Parameters of the model to be optimized.\n",
    "# - lr=0.001: Learning rate for the optimizer.\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Define the loss function for training.\n",
    "# - nn.CrossEntropyLoss: Cross-entropy loss, commonly used for classification tasks.\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Split the dataset into training and testing subsets.\n",
    "# - dataset[:len(dataset) // 10]: Take the first 10% of the dataset as the test dataset.\n",
    "# - dataset[len(dataset) // 10:]: Take the remaining 90% of the dataset as the training dataset.\n",
    "test_dataset = dataset[:len(dataset) // 10]\n",
    "train_dataset = dataset[len(dataset) // 10:]\n",
    "\n",
    "# Create DataLoader objects for batching the training and testing datasets.\n",
    "# - DataLoader: Utility for batching and shuffling data during training/testing.\n",
    "test_loader = DataLoader(test_dataset)\n",
    "train_loader = DataLoader(train_dataset)\n",
    "\n",
    "# Define the training function.\n",
    "def train():\n",
    "    \"\"\"\n",
    "    Trains the model for one epoch.\n",
    "\n",
    "    Steps:\n",
    "    1. Set the model to training mode.\n",
    "    2. Zero out the gradients of the optimizer.\n",
    "    3. Compute the loss for the training data and backpropagate the gradients.\n",
    "    4. Update the model parameters using the optimizer.\n",
    "    \"\"\"\n",
    "    model.train()  # Set the model to training mode.\n",
    "    optimizer.zero_grad()  # Clear the gradients of all optimized parameters.\n",
    "    # Compute the loss for the training data:\n",
    "    # - model()[data.train_mask]: Predictions for the training nodes.\n",
    "    # - data.y[data.train_mask]: Ground truth labels for the training nodes.\n",
    "    loss = loss_fn(model()[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()  # Backpropagate the gradients.\n",
    "    optimizer.step()  # Perform a single optimization step.\n",
    "\n",
    "# Define the testing function.\n",
    "def test():\n",
    "    \"\"\"\n",
    "    Evaluates the model on the training, validation, and test sets.\n",
    "\n",
    "    Steps:\n",
    "    1. Set the model to evaluation mode.\n",
    "    2. Compute the logits (predictions) for all nodes.\n",
    "    3. For each mask (train, validation, test):\n",
    "       - Compute the predicted class labels.\n",
    "       - Compute the accuracy by comparing predictions to ground truth labels.\n",
    "    4. Return the accuracies for the train, validation, and test sets.\n",
    "\n",
    "    Returns:\n",
    "        list: A list containing the accuracies for the train, validation, and test sets.\n",
    "    \"\"\"\n",
    "    model.eval()  # Set the model to evaluation mode.\n",
    "    logits, accs = model(), []  # Compute the logits (predictions) for all nodes.\n",
    "    # Iterate over the masks for training, validation, and testing.\n",
    "    for _, mask in data('train_mask', 'val_mask', 'test_mask'):\n",
    "        # Compute the predicted class labels for the nodes in the current mask.\n",
    "        pred = logits[mask].max(1)[1]  # Take the class with the highest probability.\n",
    "        # Compute the accuracy for the current mask.\n",
    "        acc = pred.eq(data.y[mask]).sum().item() / mask.sum().item()\n",
    "        accs.append(acc)  # Append the accuracy to the list.\n",
    "    return accs  # Return the list of accuracies.\n",
    "\n",
    "# Train the model for 50 epochs.\n",
    "for epoch in range(1, 51):\n",
    "    train()  # Train the model for one epoch.\n",
    "    accs = test()  # Evaluate the model on the train, validation, and test sets.\n",
    "    train_acc = accs[0]  # Training accuracy.\n",
    "    val_acc = accs[1]  # Validation accuracy.\n",
    "    test_acc = accs[2]  # Test accuracy.\n",
    "    # Print the accuracies for the current epoch.\n",
    "    print('Epoch: {:03d}, Train Acc: {:.5f}, '\n",
    "          'Val Acc: {:.5f}, Test Acc: {:.5f}'.format(epoch, train_acc,\n",
    "                                                     val_acc, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
